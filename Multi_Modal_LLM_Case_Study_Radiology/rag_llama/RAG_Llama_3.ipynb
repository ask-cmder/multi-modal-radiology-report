{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "950d87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "import chromadb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27bb2d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Augmented Prompt ########\n",
      "Context: The MRI of the knee demonstrates a tear in the anterior cruciate ligament (ACL).\n",
      "\n",
      "Question: What does a CT scan of the abdomen reveal?\n",
      "Answer:\n",
      "######## Response from Radiology Department is ########\n",
      " Based on the context, I would expect that a patient with an ACL injury and requiring further imaging would have multiple injuries sustained during the traumatic event, including potential internal injuries to the abdomen.\n",
      "\n",
      "Therefore, a CT scan of the abdomen may reveal:\n",
      "\n",
      "* Hemorrhage or blood in the abdominal cavity\n",
      "* Free fluid in the peritoneal space (indicating bleeding)\n",
      "* Liver or spleen lacerations\n",
      "* Fractures or injuries to the ribs or pelvic bones\n",
      "* Potential injuries to internal organs such as the intestines, kidneys, or liver\n",
      "\n",
      "Please note that this is a hypothetical scenario, and actual CT scan findings can vary greatly depending on the specific circumstances of the patient's injury.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "import chromadb\n",
    "import os\n",
    "\n",
    "# Define the LLM model to be used\n",
    "llm_model = \"llama3.2\"\n",
    "\n",
    "# Configure ChromaDB\n",
    "# Initialize the ChromaDB client with persistent storage in the current directory\n",
    "chroma_client = chromadb.PersistentClient(path=os.path.join(os.getcwd(), \"chroma_db\"))\n",
    "\n",
    "# Define a custom embedding function for ChromaDB using Ollama\n",
    "class ChromaDBEmbeddingFunction:\n",
    "    \"\"\"\n",
    "    Custom embedding function for ChromaDB using embeddings from Ollama.\n",
    "    \"\"\"\n",
    "    def __init__(self, langchain_embeddings):\n",
    "        self.langchain_embeddings = langchain_embeddings\n",
    "\n",
    "    def __call__(self, input):\n",
    "        # Ensure the input is in a list format for processing\n",
    "        if isinstance(input, str):\n",
    "            input = [input]\n",
    "        return self.langchain_embeddings.embed_documents(input)\n",
    "\n",
    "# Initialize the embedding function with Ollama embeddings\n",
    "embedding = ChromaDBEmbeddingFunction(\n",
    "    OllamaEmbeddings(\n",
    "        model=llm_model,\n",
    "        base_url=\"http://localhost:11434\"  # Adjust the base URL as per your Ollama server configuration\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define a collection for the RAG workflow\n",
    "collection_name = \"radiology_rag_collection\"\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"description\": \"A collection for RAG with Ollama - Radiology Documents\"},\n",
    "    embedding_function=embedding  # Use the custom embedding function\n",
    ")\n",
    "\n",
    "# Function to add documents to the ChromaDB collection\n",
    "def add_documents_to_collection(documents, ids):\n",
    "    \"\"\"\n",
    "    Add documents to the ChromaDB collection.\n",
    "    \n",
    "    Args:\n",
    "        documents (list of str): The documents to add.\n",
    "        ids (list of str): Unique IDs for the documents.\n",
    "    \"\"\"\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        ids=ids\n",
    "    )\n",
    "\n",
    "# Example: Radiology-related documents\n",
    "documents = [\n",
    "    \"Chest X-ray shows a consolidation in the right lower lobe, indicative of pneumonia.\",\n",
    "    \"A CT scan of the abdomen reveals a mass in the liver, consistent with a hepatic tumor.\",\n",
    "    \"The MRI of the knee demonstrates a tear in the anterior cruciate ligament (ACL).\",\n",
    "    \"Chest radiograph shows bilateral pleural effusion and signs of congestive heart failure.\",\n",
    "    \"CT angiography indicates a blockage in the left coronary artery, requiring intervention.\"\n",
    "]\n",
    "\n",
    "doc_ids = [\"kb1\", \"kb2\", \"kb3\", \"kb4\", \"kb5\"]\n",
    "\n",
    "# Documents only need to be added once or whenever an update is required. \n",
    "# This line of code is included for demonstration purposes:\n",
    "add_documents_to_collection(documents, doc_ids)\n",
    "\n",
    "# Function to query the ChromaDB collection\n",
    "def query_chromadb(query_text, n_results=1):\n",
    "    \"\"\"\n",
    "    Query the ChromaDB collection for relevant documents.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): The input query.\n",
    "        n_results (int): The number of top results to return.\n",
    "    \n",
    "    Returns:\n",
    "        list of dict: The top matching documents and their metadata.\n",
    "    \"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query_text],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    return results[\"documents\"], results[\"metadatas\"]\n",
    "\n",
    "# Function to interact with the Ollama LLM\n",
    "def query_ollama(prompt):\n",
    "    \"\"\"\n",
    "    Send a query to Ollama and retrieve the response.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The input prompt for Ollama.\n",
    "    \n",
    "    Returns:\n",
    "        str: The response from Ollama.\n",
    "    \"\"\"\n",
    "    llm = OllamaLLM(model=llm_model)\n",
    "    return llm.invoke(prompt)\n",
    "\n",
    "# RAG pipeline: Combine ChromaDB and Ollama for Retrieval-Augmented Generation\n",
    "def rag_pipeline(query_text):\n",
    "    \"\"\"\n",
    "    Perform Retrieval-Augmented Generation (RAG) by combining ChromaDB and Ollama.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): The input query.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated response from Ollama augmented with retrieved context.\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents from ChromaDB\n",
    "    retrieved_docs, metadata = query_chromadb(query_text)\n",
    "    context = \" \".join(retrieved_docs[0]) if retrieved_docs else \"No relevant documents found.\"\n",
    "\n",
    "    # Step 2: Send the query along with the context to Ollama\n",
    "    augmented_prompt = f\"Context: {context}\\n\\nQuestion: {query_text}\\nAnswer:\"\n",
    "    print(\"######## Augmented Prompt ########\")\n",
    "    print(augmented_prompt)\n",
    "\n",
    "    response = query_ollama(augmented_prompt)\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "# Define a query to test the RAG pipeline\n",
    "query = \"What does a CT scan of the abdomen reveal?\"  # Change the query as needed\n",
    "response = rag_pipeline(query)\n",
    "print(\"######## Response from Radiology Department is ########\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24ab3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
